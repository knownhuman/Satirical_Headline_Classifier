{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85b2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:25:16.837428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# loading the necessary libraries\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b05d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then load the data\n",
    "\n",
    "df = pd.read_json('sarcasm.json')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8211e662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Article link is unnecessary, so drop that and then check the work. \n",
    "NOTE - when dropping, remember to save the work into a variable name, else Pandas will just\n",
    "do the work in place and you'll still have that column you thought you removed.\n",
    "That totally didn't happen to me.\n",
    "'''\n",
    "\n",
    "df = df.drop(['article_link'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaffa4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43895316185555433 26709\n"
     ]
    }
   ],
   "source": [
    "# Checking data distribution\n",
    "\n",
    "is_sarcastic = df['is_sarcastic'].value_counts()[1]\n",
    "print(is_sarcastic/len(df), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c42314",
   "metadata": {},
   "source": [
    "### 44% of the headlines are satirical, which is a fairly even data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbc536",
   "metadata": {},
   "source": [
    "## Train / Test / Validate Split\n",
    "\n",
    "The tutorial I'm using for this only focuses on a Train/Test split, but I'm hoping to do some tweaking and test the accuracy a bit. So, for that I'm going to divide into three groups. First, I want a validation set, a decent enough group to test against my final trained model just to see how it performs with brand new data. \n",
    "\n",
    "Then, I want a test set, which I will use to see how well each model iteration learns. We have a decent amount of data here (26k items for a binary classifier) so I'm holding out 20% for the Validation set, and then of the remaining 80, an equal number for testing (25%) and training (75%) which should give me:\n",
    "\n",
    "Validation - 5342\n",
    "Testing - 5342\n",
    "Training - 16025\n",
    "\n",
    "If the model has trouble learning, I can tweak those down to 15/15/70 or even 12/12/76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e080e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54afbeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21367 5342\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train_full), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd95b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5342 5342 16025\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "print(len(df_val), len(df_test), len(df_train))\n",
    "\n",
    "# Checking my math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4fc3a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18631</th>\n",
       "      <td>uncle strikes out hard with book gift</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24365</th>\n",
       "      <td>cities in this state have the worst smog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>the world economic forum is giving goosebumps ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23860</th>\n",
       "      <td>period of time in which parents proud of how m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11689</th>\n",
       "      <td>why shrimp scampi has been on america's mind a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "18631              uncle strikes out hard with book gift             1\n",
       "24365           cities in this state have the worst smog             0\n",
       "8924   the world economic forum is giving goosebumps ...             0\n",
       "23860  period of time in which parents proud of how m...             1\n",
       "11689  why shrimp scampi has been on america's mind a...             0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing this so that after I drop the target variable from the training \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e7eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.is_sarcastic.values\n",
    "y_val = df_val.is_sarcastic.values\n",
    "\n",
    "del df_train['is_sarcastic']\n",
    "del df_val['is_sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3963005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncle strikes out hard with book gift value:  1\n",
      "cities in this state have the worst smog value:  0\n",
      "the world economic forum is giving goosebumps to some 'game of thrones' fans value:  0\n",
      "period of time in which parents proud of how much child can eat quickly dwindling value:  1\n",
      "why shrimp scampi has been on america's mind all week value:  0\n"
     ]
    }
   ],
   "source": [
    "# with these separated out, just making sure that things line up\n",
    "\n",
    "for i in range(5):\n",
    "    print(df_train.iloc[i]['headline'], \"value: \", y_train.item(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28a6fb",
   "metadata": {},
   "source": [
    "The values match. So, we're good to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce872a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "oov_tok = \"<oov>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(df_train['headline'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train['headline'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen = max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(df_val['headline'])\n",
    "val_padded = pad_sequences(val_sequences, maxlen = max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca665f",
   "metadata": {},
   "source": [
    "Had to modify the tutorial again since I am working with a dataframe instead of a list, but we have tokenized values. \n",
    "\n",
    "Of note, I had to specify the column I wanted to tokenize else things got wonky. Also, running locally the tokenizing process happened so quickly that I expected it didn't work and spent several minutes before finally out putting the final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd1eba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:27:04.030790: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-02 11:27:04.061497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-02 11:27:04.403676: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:04.426794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:10:00.0 name: NVIDIA GeForce GTX 1060 3GB computeCapability: 6.1\n",
      "coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2023-01-02 11:27:04.426867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-02 11:27:05.363515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-02 11:27:05.363709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-02 11:27:05.860309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-02 11:27:05.994299: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-02 11:27:06.864102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-02 11:27:07.059414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-02 11:27:08.655018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-02 11:27:08.656164: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:08.657061: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:08.657100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-02 11:27:08.657590: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-02 11:27:08.659017: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-02 11:27:08.659906: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:08.659946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:10:00.0 name: NVIDIA GeForce GTX 1060 3GB computeCapability: 6.1\n",
      "coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2023-01-02 11:27:08.659999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-02 11:27:08.660051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-02 11:27:08.660064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-02 11:27:08.660076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-02 11:27:08.660087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-02 11:27:08.660099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-02 11:27:08.660109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-02 11:27:08.660120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-02 11:27:08.660640: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:08.661139: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:08.661160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-02 11:27:08.695701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-02 11:27:25.466974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-02 11:27:25.467014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-01-02 11:27:25.467020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-01-02 11:27:25.492971: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:25.493012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-02 11:27:25.493706: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:25.494317: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:10:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-02 11:27:25.494372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2067 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 3GB, pci bus id: 0000:10:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376376bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert trainied lists to numpy arrays for Tensorflow\n",
    "\n",
    "train_padded = np.array(train_padded)\n",
    "y_train = np.array(y_train)\n",
    "val_padded = np.array(val_padded)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a88b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:27:36.548080: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-01-02 11:27:36.854609: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593145000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:27:42.946369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501/501 - 78s - loss: 0.6805 - accuracy: 0.5604 - val_loss: 0.6607 - val_accuracy: 0.5610\n",
      "Epoch 2/8\n",
      "501/501 - 7s - loss: 0.5255 - accuracy: 0.7574 - val_loss: 0.4280 - val_accuracy: 0.8160\n",
      "Epoch 3/8\n",
      "501/501 - 7s - loss: 0.3411 - accuracy: 0.8681 - val_loss: 0.3765 - val_accuracy: 0.8272\n",
      "Epoch 4/8\n",
      "501/501 - 6s - loss: 0.2780 - accuracy: 0.8929 - val_loss: 0.3421 - val_accuracy: 0.8486\n",
      "Epoch 5/8\n",
      "501/501 - 8s - loss: 0.2363 - accuracy: 0.9120 - val_loss: 0.3393 - val_accuracy: 0.8502\n",
      "Epoch 6/8\n",
      "501/501 - 8s - loss: 0.2033 - accuracy: 0.9266 - val_loss: 0.3401 - val_accuracy: 0.8519\n",
      "Epoch 7/8\n",
      "501/501 - 9s - loss: 0.1778 - accuracy: 0.9366 - val_loss: 0.3543 - val_accuracy: 0.8502\n",
      "Epoch 8/8\n",
      "501/501 - 8s - loss: 0.1545 - accuracy: 0.9451 - val_loss: 0.3604 - val_accuracy: 0.8512\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "num_epochs = 8\n",
    "\n",
    "history = model.fit(train_padded, y_train, epochs=num_epochs, validation_data=(val_padded, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fac9e",
   "metadata": {},
   "source": [
    "Now validation accuracy is right at 85, rather than the ~80% we were seeing with longer training. Feel good about stopping there for the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baabc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"satire_headline.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e784526",
   "metadata": {},
   "source": [
    "## Same Task, But with NLTK\n",
    "\n",
    "Deploying Tensorflow is a bit of a bear. I'm going to follow the book and try and get Tensorflow Lite running on Django, but it's clear that I need to learn Tensorflow Serving to really get Tensorflow in production. \n",
    "\n",
    "**However** what is clear is that Tensorflow also might be overkill for a pretty basic NLP-focused binary classification system. Towards that end, I want to do the same thing over, this time using the Natural Language Tool Kit (NLTK). \n",
    "\n",
    "At just 3.8MB all day, the NLTK is >10 times smaller than TF Lite and >300 times smaller than Tensorflow. \n",
    "\n",
    "Let's see what kind of accuracy I can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67b289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''It looks like NLTK works based on a list of tuples. So, my first step it turn my nice modern dataframe into a list \n",
    "of tuples.\n",
    "\n",
    "But first, I want to convert the labels into words. \n",
    "'''\n",
    "\n",
    "df2 = df\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a2c6f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>not_satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>not_satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>not_satire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...   not_satire\n",
       "1  the 'roseanne' revival catches up to our thorn...   not_satire\n",
       "2  mom starting to fear son's web series closest ...       satire\n",
       "3  boehner just wants wife to listen, not come up...       satire\n",
       "4  j.k. rowling wishes snape happy birthday in th...   not_satire"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['is_sarcastic'] = df2['is_sarcastic'].replace(to_replace=[0,1], value=[\"not_satire\",\"satire\"])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4314a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"former versace store clerk sues over secret 'black code' for minority shoppers\", 'not_satire'), (\"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", 'not_satire'), (\"mom starting to fear son's web series closest thing she will have to grandchild\", 'satire'), ('boehner just wants wife to listen, not come up with alternative debt-reduction ideas', 'satire'), ('j.k. rowling wishes snape happy birthday in the most magical way', 'not_satire')]\n"
     ]
    }
   ],
   "source": [
    "headlines = list(df.itertuples(index=False, name=None))\n",
    "print(headlines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ca040",
   "metadata": {},
   "source": [
    "Well, that was easy. Now to build a NLTK-powered Naive Bayes classifier. \n",
    "\n",
    "First step, build the token libary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e20c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/knownhuman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26709\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_features(text):\n",
    "    features = {}\n",
    "    word_list = [word for word in word_tokenize(text.lower())]\n",
    "    for word in word_list:\n",
    "        features[word] = True\n",
    "    return features \n",
    "\n",
    "all_features = [(get_features(headline), label) for (headline, label) in headlines]\n",
    "\n",
    "print(len(all_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01a1595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 21367 headlines\n",
      "Test set size = 5342 headlines\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier, classify\n",
    "\n",
    "def train(features, proportion):\n",
    "    train_size = int(len(features) * proportion)\n",
    "    # Building training and test sets on the fly\n",
    "    train_set, test_set = features[:train_size], features[train_size:]\n",
    "    print(f\"Training set size = {str(len(train_set))} headlines\")\n",
    "    print(f\"Test set size = {str(len(test_set))} headlines\")\n",
    "    # training\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return train_set, test_set, classifier\n",
    "\n",
    "train_set, test_set, classifier = train(all_features, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1b2682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy = 0.9517012215098049\n",
      "Test Set Accuracy = 0.8614751029576937\n",
      "Most Informative Features\n",
      "                    area = True           satire : not_sa =     56.4 : 1.0\n",
      "                  hoping = True           satire : not_sa =     28.3 : 1.0\n",
      "                  donald = True           not_sa : satire =     27.8 : 1.0\n",
      "                       % = True           satire : not_sa =     26.0 : 1.0\n",
      "                    2015 = True           not_sa : satire =     25.0 : 1.0\n",
      "                 reasons = True           not_sa : satire =     24.4 : 1.0\n",
      "                  muslim = True           not_sa : satire =     22.3 : 1.0\n",
      "                 protest = True           not_sa : satire =     22.3 : 1.0\n",
      "                  nation = True           satire : not_sa =     21.8 : 1.0\n",
      "                  slowly = True           satire : not_sa =     21.6 : 1.0\n",
      "                 elderly = True           satire : not_sa =     20.7 : 1.0\n",
      "                  allows = True           satire : not_sa =     19.9 : 1.0\n",
      "                 assures = True           satire : not_sa =     19.9 : 1.0\n",
      "             transgender = True           not_sa : satire =     18.6 : 1.0\n",
      "                coworker = True           satire : not_sa =     18.5 : 1.0\n",
      "            disappointed = True           satire : not_sa =     16.5 : 1.0\n",
      "                whatever = True           satire : not_sa =     16.5 : 1.0\n",
      "                 colbert = True           not_sa : satire =     16.0 : 1.0\n",
      "              frustrated = True           satire : not_sa =     15.7 : 1.0\n",
      "               apartment = True           satire : not_sa =     15.0 : 1.0\n",
      "              previously = True           satire : not_sa =     14.8 : 1.0\n",
      "                 excited = True           satire : not_sa =     14.5 : 1.0\n",
      "                       # = True           not_sa : satire =     14.1 : 1.0\n",
      "                announce = True           satire : not_sa =     14.0 : 1.0\n",
      "              introduces = True           satire : not_sa =     13.5 : 1.0\n",
      "                     few = True           satire : not_sa =     13.4 : 1.0\n",
      "               allegedly = True           not_sa : satire =     13.4 : 1.0\n",
      "                   onion = True           satire : not_sa =     13.1 : 1.0\n",
      "                 section = True           satire : not_sa =     13.1 : 1.0\n",
      "                 lessons = True           not_sa : satire =     13.1 : 1.0\n",
      "                   viral = True           not_sa : satire =     12.9 : 1.0\n",
      "                     bar = True           satire : not_sa =     12.8 : 1.0\n",
      "                audience = True           satire : not_sa =     12.5 : 1.0\n",
      "                   local = True           satire : not_sa =     12.4 : 1.0\n",
      "                  france = True           not_sa : satire =     12.3 : 1.0\n",
      "              reportedly = True           not_sa : satire =     11.8 : 1.0\n",
      "                 recipes = True           not_sa : satire =     11.8 : 1.0\n",
      "                   these = True           not_sa : satire =     11.7 : 1.0\n",
      "                     dvd = True           satire : not_sa =     11.4 : 1.0\n",
      "               horrified = True           satire : not_sa =     11.4 : 1.0\n",
      "                   laden = True           satire : not_sa =     11.4 : 1.0\n",
      "                position = True           satire : not_sa =     11.4 : 1.0\n",
      "                    self = True           satire : not_sa =     11.4 : 1.0\n",
      "                    uber = True           not_sa : satire =     11.3 : 1.0\n",
      "                     yes = True           not_sa : satire =     11.3 : 1.0\n",
      "                    2014 = True           not_sa : satire =     11.2 : 1.0\n",
      "                  tweets = True           not_sa : satire =     11.1 : 1.0\n",
      "              historical = True           satire : not_sa =     11.1 : 1.0\n",
      "                  annual = True           satire : not_sa =     10.9 : 1.0\n",
      "                 nominee = True           not_sa : satire =     10.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(train_set, test_set, classifier):\n",
    "    print(f\"Training Set Accuracy = {str(classify.accuracy(classifier, train_set))}\")\n",
    "    print(f\"Test Set Accuracy = {str(classify.accuracy(classifier, test_set))}\")\n",
    "    classifier.show_most_informative_features(50)\n",
    "          \n",
    "evaluate(train_set, test_set, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ff550",
   "metadata": {},
   "source": [
    "Well damn, 86% accuracy in a training time of seconds. Let's see how this performs in real life. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9be1983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in your headline: this is a test headline\n",
      "This headline is likely not_satire\n",
      "\n",
      "Type in your headline: Area man hoping his team really pulls it off this year\n",
      "This headline is likely satire\n",
      "\n",
      "Type in your headline: Invite the neighbors\n",
      "This headline is likely not_satire\n",
      "\n",
      "Type in your headline: \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    headline = input(\"Type in your headline: \")\n",
    "    if len(headline)==0:\n",
    "        break\n",
    "    else:\n",
    "        prediction = classifier.classify(get_features(headline))\n",
    "        print(f\"This headline is likely {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30109c5e",
   "metadata": {},
   "source": [
    "Based on a small sample size, the model performed better with real news than it did with satire, and if definitely picked out some of the most common words used by the Onion. \n",
    "\n",
    "Alright, time to pickle this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6731fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('satire_classifier.pickle', 'wb') as out:\n",
    "    pickle.dump(classifier, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
